{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c 'https://raw.githubusercontent.com/omyfish/deep-learning/master/Char%20RNN/data/anna.txt'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every unhappy family is unhappy in its own\n",
      "way.\n",
      "\n",
      "Everythin\n"
     ]
    }
   ],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Create a couple dictionaries to convert char to and from integers. Encoding characters as integers make it easier to used as input in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "\n",
    "char = tuple(set(text)) ## this will read out the distinct character\n",
    "int2char = dict(enumerate(char)) ## maps integers to characters\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "#encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([79, 49, 16, 75, 62, 42, 68, 17, 57, 81, 81, 81, 47, 16, 75, 75,  1,\n",
       "       17, 11, 16, 58, 56, 14, 56, 42, 63, 17, 16, 68, 42, 17, 16, 14, 14,\n",
       "       17, 16, 14, 56, 36, 42, 82, 17, 42, 69, 42, 68,  1, 17, 52, 39, 49,\n",
       "       16, 75, 75,  1, 17, 11, 16, 58, 56, 14,  1, 17, 56, 63, 17, 52, 39,\n",
       "       49, 16, 75, 75,  1, 17, 56, 39, 17, 56, 62, 63, 17, 21, 50, 39, 81,\n",
       "       50, 16,  1, 78, 81, 81, 43, 69, 42, 68,  1, 62, 49, 56, 39])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data\n",
    "Our LSTM expect an input that is One-hot-encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    #initiate the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    #fill the appropriate element with 1    \n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    #finally, reshape it back to the orignal array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([3,2,1])\n",
    "one_hot = one_hot_encode(arr, 8)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    num_batch_total = batch_size * seq_length\n",
    "    n_batches = len(arr) // num_batch_total\n",
    "    arr = arr[:(n_batches * num_batch_total)]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:,:-1],y[:,-1] = x[:,1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:,:-1],y[:,-1] = x[:,1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[79 49 16 75 62 42 68 17 57 81]\n",
      " [63 21 39 17 62 49 16 62 17 16]\n",
      " [42 39 22 17 21 68 17 16 17 11]\n",
      " [63 17 62 49 42 17 59 49 56 42]\n",
      " [17 63 16 50 17 49 42 68 17 62]\n",
      " [59 52 63 63 56 21 39 17 16 39]\n",
      " [17 60 39 39 16 17 49 16 22 17]\n",
      " [38  3 14 21 39 63 36  1 78 17]]\n",
      "\n",
      "y\n",
      " [[49 16 75 62 42 68 17 57 81 81]\n",
      " [21 39 17 62 49 16 62 17 16 62]\n",
      " [39 22 17 21 68 17 16 17 11 21]\n",
      " [17 62 49 42 17 59 49 56 42 11]\n",
      " [63 16 50 17 49 42 68 17 62 42]\n",
      " [52 63 63 56 21 39 17 16 39 22]\n",
      " [60 39 39 16 17 49 16 22 17 63]\n",
      " [ 3 14 21 39 63 36  1 78 17 67]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=125, n_layers=2, drop_prop=0.5, lr = 0.01):\n",
    "        super().__init__()\n",
    "        # hyp\n",
    "        self.drop_prop = drop_prop\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.lr = lr\n",
    "        \n",
    "        #creating char dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(tokens))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        # define layer of the mdel\n",
    "        self.lstm = nn.LSTM(input_size = len(tokens), hidden_size = n_hidden,num_layers = n_layers, dropout = drop_prop, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prop)\n",
    "        self.fc = nn.Linear(n_hidden, len(tokens))\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.lstm(x, hidden)\n",
    "        r_out = self.dropout(r_out)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = r_out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 125, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=125, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokens = tuple(set(text))\n",
    "n_hidden = 512\n",
    "n_layers = 2\n",
    "dropoutrate = 0.5\n",
    "net = CharRNN(tokens)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train\n",
    "\n",
    "The train function gives us the ability to set the number of epochs, the learning rate, and other parameters.\n",
    "\n",
    "Below we're using an Adam optimizer and cross entropy loss since we are looking at character class scores as output. We calculate the loss and perform backpropagation, as usual!\n",
    "\n",
    "A couple of details about training: \n",
    ">* Within the batch loop, we detach the hidden state from its history; this time setting it equal to a new *tuple* variable because an LSTM has a hidden state that is a tuple of the hidden and cell states.\n",
    "* We use [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) to help prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(net, data, epochs=10, batch_size=10,seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "\n",
    "    net: CharRNN network\n",
    "    data: text data to train the network\n",
    "    epochs: Number of epochs to train\n",
    "    batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "    seq_length: Number of character steps per mini-batch\n",
    "    lr: learning rate\n",
    "    clip: gradient clipping\n",
    "    val_frac: Fraction of data to hold out for validation\n",
    "    print_every: Number of steps for printing training and validation loss\n",
    "\n",
    "    '''    \n",
    "    net.train()\n",
    "    optimizer = optim.Adam(net.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation dat\n",
    "    val_idx = int(len(data) * (1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if (train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    count = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            count += 1\n",
    "            #One-hot encode our data and make them torch tensor\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if (train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            targets = targets.view(batch_size*seq_length).long()\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if count % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    if (train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                net.train() # reset to train mode after iterating the validation \n",
    "                \n",
    "                print('Epochs {}/{}'.format(e+1, epochs),\n",
    "                     'Step: {}'.format(count),\n",
    "                     'Loss: {:.4f}...'.format(loss.item()),\n",
    "                     'Validation Loss {:.4f}...'.format(np.mean(val_losses)))\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1/20 Step: 10 Loss: 3.9972... Validation Loss 3.8030...\n",
      "Epochs 1/20 Step: 20 Loss: 3.2697... Validation Loss 3.1886...\n",
      "Epochs 1/20 Step: 30 Loss: 3.2344... Validation Loss 3.1570...\n",
      "Epochs 1/20 Step: 40 Loss: 3.1879... Validation Loss 3.1466...\n",
      "Epochs 1/20 Step: 50 Loss: 3.2035... Validation Loss 3.1420...\n",
      "Epochs 1/20 Step: 60 Loss: 3.1801... Validation Loss 3.1396...\n",
      "Epochs 1/20 Step: 70 Loss: 3.1587... Validation Loss 3.1377...\n",
      "Epochs 1/20 Step: 80 Loss: 3.1661... Validation Loss 3.1357...\n",
      "Epochs 1/20 Step: 90 Loss: 3.1726... Validation Loss 3.1342...\n",
      "Epochs 1/20 Step: 100 Loss: 3.1571... Validation Loss 3.1326...\n",
      "Epochs 1/20 Step: 110 Loss: 3.1551... Validation Loss 3.1312...\n",
      "Epochs 1/20 Step: 120 Loss: 3.1350... Validation Loss 3.1297...\n",
      "Epochs 1/20 Step: 130 Loss: 3.1532... Validation Loss 3.1276...\n",
      "Epochs 2/20 Step: 140 Loss: 3.1307... Validation Loss 3.1256...\n",
      "Epochs 2/20 Step: 150 Loss: 3.1448... Validation Loss 3.1233...\n",
      "Epochs 2/20 Step: 160 Loss: 3.1345... Validation Loss 3.1203...\n",
      "Epochs 2/20 Step: 170 Loss: 3.1026... Validation Loss 3.1163...\n",
      "Epochs 2/20 Step: 180 Loss: 3.1094... Validation Loss 3.1110...\n",
      "Epochs 2/20 Step: 190 Loss: 3.1066... Validation Loss 3.1017...\n",
      "Epochs 2/20 Step: 200 Loss: 3.0964... Validation Loss 3.0860...\n",
      "Epochs 2/20 Step: 210 Loss: 3.0803... Validation Loss 3.0604...\n",
      "Epochs 2/20 Step: 220 Loss: 3.0544... Validation Loss 3.0254...\n",
      "Epochs 2/20 Step: 230 Loss: 3.0157... Validation Loss 2.9880...\n",
      "Epochs 2/20 Step: 240 Loss: 2.9807... Validation Loss 2.9444...\n",
      "Epochs 2/20 Step: 250 Loss: 2.9399... Validation Loss 2.9057...\n",
      "Epochs 2/20 Step: 260 Loss: 2.9063... Validation Loss 2.8696...\n",
      "Epochs 2/20 Step: 270 Loss: 2.8720... Validation Loss 2.8368...\n",
      "Epochs 3/20 Step: 280 Loss: 2.8771... Validation Loss 2.8031...\n",
      "Epochs 3/20 Step: 290 Loss: 2.8315... Validation Loss 2.7785...\n",
      "Epochs 3/20 Step: 300 Loss: 2.8247... Validation Loss 2.7440...\n",
      "Epochs 3/20 Step: 310 Loss: 2.8045... Validation Loss 2.7162...\n",
      "Epochs 3/20 Step: 320 Loss: 2.7796... Validation Loss 2.6920...\n",
      "Epochs 3/20 Step: 330 Loss: 2.7384... Validation Loss 2.6724...\n",
      "Epochs 3/20 Step: 340 Loss: 2.7207... Validation Loss 2.6575...\n",
      "Epochs 3/20 Step: 350 Loss: 2.7269... Validation Loss 2.6381...\n",
      "Epochs 3/20 Step: 360 Loss: 2.6883... Validation Loss 2.6217...\n",
      "Epochs 3/20 Step: 370 Loss: 2.6936... Validation Loss 2.6065...\n",
      "Epochs 3/20 Step: 380 Loss: 2.6799... Validation Loss 2.5931...\n",
      "Epochs 3/20 Step: 390 Loss: 2.6578... Validation Loss 2.5840...\n",
      "Epochs 3/20 Step: 400 Loss: 2.6471... Validation Loss 2.5681...\n",
      "Epochs 3/20 Step: 410 Loss: 2.6366... Validation Loss 2.5581...\n",
      "Epochs 4/20 Step: 420 Loss: 2.6257... Validation Loss 2.5519...\n",
      "Epochs 4/20 Step: 430 Loss: 2.6122... Validation Loss 2.5405...\n",
      "Epochs 4/20 Step: 440 Loss: 2.6203... Validation Loss 2.5309...\n",
      "Epochs 4/20 Step: 450 Loss: 2.5698... Validation Loss 2.5226...\n",
      "Epochs 4/20 Step: 460 Loss: 2.5710... Validation Loss 2.5166...\n",
      "Epochs 4/20 Step: 470 Loss: 2.5734... Validation Loss 2.5096...\n",
      "Epochs 4/20 Step: 480 Loss: 2.5658... Validation Loss 2.5038...\n",
      "Epochs 4/20 Step: 490 Loss: 2.5713... Validation Loss 2.4973...\n",
      "Epochs 4/20 Step: 500 Loss: 2.5630... Validation Loss 2.4912...\n",
      "Epochs 4/20 Step: 510 Loss: 2.5759... Validation Loss 2.4858...\n",
      "Epochs 4/20 Step: 520 Loss: 2.5659... Validation Loss 2.4800...\n",
      "Epochs 4/20 Step: 530 Loss: 2.5333... Validation Loss 2.4748...\n",
      "Epochs 4/20 Step: 540 Loss: 2.5151... Validation Loss 2.4697...\n",
      "Epochs 4/20 Step: 550 Loss: 2.5298... Validation Loss 2.4663...\n",
      "Epochs 5/20 Step: 560 Loss: 2.5052... Validation Loss 2.4588...\n",
      "Epochs 5/20 Step: 570 Loss: 2.5128... Validation Loss 2.4541...\n",
      "Epochs 5/20 Step: 580 Loss: 2.4994... Validation Loss 2.4483...\n",
      "Epochs 5/20 Step: 590 Loss: 2.5052... Validation Loss 2.4457...\n",
      "Epochs 5/20 Step: 600 Loss: 2.4731... Validation Loss 2.4386...\n",
      "Epochs 5/20 Step: 610 Loss: 2.4977... Validation Loss 2.4327...\n",
      "Epochs 5/20 Step: 620 Loss: 2.4708... Validation Loss 2.4278...\n",
      "Epochs 5/20 Step: 630 Loss: 2.5061... Validation Loss 2.4227...\n",
      "Epochs 5/20 Step: 640 Loss: 2.4858... Validation Loss 2.4174...\n",
      "Epochs 5/20 Step: 650 Loss: 2.4746... Validation Loss 2.4121...\n",
      "Epochs 5/20 Step: 660 Loss: 2.4481... Validation Loss 2.4066...\n",
      "Epochs 5/20 Step: 670 Loss: 2.4848... Validation Loss 2.4002...\n",
      "Epochs 5/20 Step: 680 Loss: 2.4890... Validation Loss 2.3941...\n",
      "Epochs 5/20 Step: 690 Loss: 2.4360... Validation Loss 2.3932...\n",
      "Epochs 6/20 Step: 700 Loss: 2.4288... Validation Loss 2.3825...\n",
      "Epochs 6/20 Step: 710 Loss: 2.4504... Validation Loss 2.3777...\n",
      "Epochs 6/20 Step: 720 Loss: 2.4155... Validation Loss 2.3724...\n",
      "Epochs 6/20 Step: 730 Loss: 2.4368... Validation Loss 2.3667...\n",
      "Epochs 6/20 Step: 740 Loss: 2.4308... Validation Loss 2.3626...\n",
      "Epochs 6/20 Step: 750 Loss: 2.3892... Validation Loss 2.3577...\n",
      "Epochs 6/20 Step: 760 Loss: 2.4249... Validation Loss 2.3518...\n",
      "Epochs 6/20 Step: 770 Loss: 2.4221... Validation Loss 2.3465...\n",
      "Epochs 6/20 Step: 780 Loss: 2.3993... Validation Loss 2.3416...\n",
      "Epochs 6/20 Step: 790 Loss: 2.4016... Validation Loss 2.3356...\n",
      "Epochs 6/20 Step: 800 Loss: 2.3940... Validation Loss 2.3323...\n",
      "Epochs 6/20 Step: 810 Loss: 2.3947... Validation Loss 2.3277...\n",
      "Epochs 6/20 Step: 820 Loss: 2.3985... Validation Loss 2.3213...\n",
      "Epochs 6/20 Step: 830 Loss: 2.4117... Validation Loss 2.3182...\n",
      "Epochs 7/20 Step: 840 Loss: 2.3556... Validation Loss 2.3133...\n",
      "Epochs 7/20 Step: 850 Loss: 2.3734... Validation Loss 2.3161...\n",
      "Epochs 7/20 Step: 860 Loss: 2.3669... Validation Loss 2.3049...\n",
      "Epochs 7/20 Step: 870 Loss: 2.3617... Validation Loss 2.3020...\n",
      "Epochs 7/20 Step: 880 Loss: 2.3634... Validation Loss 2.2967...\n",
      "Epochs 7/20 Step: 890 Loss: 2.3556... Validation Loss 2.2934...\n",
      "Epochs 7/20 Step: 900 Loss: 2.3333... Validation Loss 2.2880...\n",
      "Epochs 7/20 Step: 910 Loss: 2.3446... Validation Loss 2.2850...\n",
      "Epochs 7/20 Step: 920 Loss: 2.3414... Validation Loss 2.2807...\n",
      "Epochs 7/20 Step: 930 Loss: 2.3261... Validation Loss 2.2779...\n",
      "Epochs 7/20 Step: 940 Loss: 2.3410... Validation Loss 2.2745...\n",
      "Epochs 7/20 Step: 950 Loss: 2.3501... Validation Loss 2.2705...\n",
      "Epochs 7/20 Step: 960 Loss: 2.3383... Validation Loss 2.2666...\n",
      "Epochs 7/20 Step: 970 Loss: 2.3416... Validation Loss 2.2683...\n",
      "Epochs 8/20 Step: 980 Loss: 2.3153... Validation Loss 2.2598...\n",
      "Epochs 8/20 Step: 990 Loss: 2.3242... Validation Loss 2.2554...\n",
      "Epochs 8/20 Step: 1000 Loss: 2.3150... Validation Loss 2.2514...\n",
      "Epochs 8/20 Step: 1010 Loss: 2.3374... Validation Loss 2.2490...\n",
      "Epochs 8/20 Step: 1020 Loss: 2.3258... Validation Loss 2.2452...\n",
      "Epochs 8/20 Step: 1030 Loss: 2.2992... Validation Loss 2.2438...\n",
      "Epochs 8/20 Step: 1040 Loss: 2.3012... Validation Loss 2.2380...\n",
      "Epochs 8/20 Step: 1050 Loss: 2.3276... Validation Loss 2.2358...\n",
      "Epochs 8/20 Step: 1060 Loss: 2.3150... Validation Loss 2.2324...\n",
      "Epochs 8/20 Step: 1070 Loss: 2.3169... Validation Loss 2.2289...\n",
      "Epochs 8/20 Step: 1080 Loss: 2.2993... Validation Loss 2.2272...\n",
      "Epochs 8/20 Step: 1090 Loss: 2.2956... Validation Loss 2.2249...\n",
      "Epochs 8/20 Step: 1100 Loss: 2.2858... Validation Loss 2.2192...\n",
      "Epochs 8/20 Step: 1110 Loss: 2.2906... Validation Loss 2.2162...\n",
      "Epochs 9/20 Step: 1120 Loss: 2.2967... Validation Loss 2.2156...\n",
      "Epochs 9/20 Step: 1130 Loss: 2.2725... Validation Loss 2.2106...\n",
      "Epochs 9/20 Step: 1140 Loss: 2.2923... Validation Loss 2.2059...\n",
      "Epochs 9/20 Step: 1150 Loss: 2.2864... Validation Loss 2.2032...\n",
      "Epochs 9/20 Step: 1160 Loss: 2.2459... Validation Loss 2.2008...\n",
      "Epochs 9/20 Step: 1170 Loss: 2.2519... Validation Loss 2.1979...\n",
      "Epochs 9/20 Step: 1180 Loss: 2.2781... Validation Loss 2.1937...\n",
      "Epochs 9/20 Step: 1190 Loss: 2.2834... Validation Loss 2.1904...\n",
      "Epochs 9/20 Step: 1200 Loss: 2.2437... Validation Loss 2.1870...\n",
      "Epochs 9/20 Step: 1210 Loss: 2.2368... Validation Loss 2.1827...\n",
      "Epochs 9/20 Step: 1220 Loss: 2.2606... Validation Loss 2.1809...\n",
      "Epochs 9/20 Step: 1230 Loss: 2.2303... Validation Loss 2.1781...\n",
      "Epochs 9/20 Step: 1240 Loss: 2.2327... Validation Loss 2.1748...\n",
      "Epochs 9/20 Step: 1250 Loss: 2.2418... Validation Loss 2.1761...\n",
      "Epochs 10/20 Step: 1260 Loss: 2.2411... Validation Loss 2.1692...\n",
      "Epochs 10/20 Step: 1270 Loss: 2.2219... Validation Loss 2.1650...\n",
      "Epochs 10/20 Step: 1280 Loss: 2.2589... Validation Loss 2.1615...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 10/20 Step: 1290 Loss: 2.2344... Validation Loss 2.1594...\n",
      "Epochs 10/20 Step: 1300 Loss: 2.2314... Validation Loss 2.1566...\n",
      "Epochs 10/20 Step: 1310 Loss: 2.2363... Validation Loss 2.1548...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-329-d34e17794f16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-328-750d5c0fb206>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, data, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[0;32m     73\u001b[0m                     \u001b[0mval_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meach\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_h\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m                     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                     \u001b[0mval_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-322-dc99b02ed157>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mr_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mr_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    562\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 564\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[1;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[1;32m--> 526\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the best model\n",
    "\n",
    "To set your hyperparameters to get the best performance, you'll want to watch the training and validation losses. If your training loss is much lower than the validation loss, you're overfitting. Increase regularization (more dropout) or use a smaller network. If the training and validation losses are close, you're underfitting so you can increase the size of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here are the hyperparameters for the network.\n",
    "\n",
    "In defining the model:\n",
    "* `n_hidden` - The number of units in the hidden layers.\n",
    "* `n_layers` - Number of hidden LSTM layers to use.\n",
    "\n",
    "We assume that dropout probability and learning rate will be kept at the default, in this example.\n",
    "\n",
    "And in training:\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `seq_length` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lr` - Learning rate for training\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to copy it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `n_hidden` and `n_layers`. I would advise that you always use `n_layers` of either 2/3. The `n_hidden` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `n_hidden` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "After training, we'll save the model so we can load it again later if we need too. Here I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Making Predictions\n",
    "\n",
    "Now that the model is trained, we'll want to sample from it and make predictions about next characters! To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. Just keep doing this and you'll generate a bunch of text!\n",
    "\n",
    "### A note on the `predict`  function\n",
    "\n",
    "The output of our RNN is from a fully-connected layer and it outputs a **distribution of next-character scores**.\n",
    "\n",
    "> To actually get the next character, we apply a softmax function, which gives us a *probability* distribution that we can then sample to predict the next character.\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "Our predictions come from a categorical probability distribution over all the possible characters. We can make the sample text and make it more reasonable to handle (with less variables) by only considering some $K$ most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text. Read more about [topk, here](https://pytorch.org/docs/stable/torch.html#torch.topk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if h == None:\n",
    "            h = net.init_hidden(1)\n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "            \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h ])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "        \n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            p = p.cuda()\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.to('cpu').numpy().squeeze()\n",
    "    \n",
    "\n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.to('cpu').numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "       \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = predict(net, 'T')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priming and generating text \n",
    "\n",
    "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "    if (train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "        \n",
    "    net.eval() #evalate net\n",
    "    \n",
    "    #First off, run throught the prime char\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k = top_k)\n",
    "        chars.append(char)\n",
    "        \n",
    "    #Now pass in the previous char and get new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "    \n",
    "    return ''.join(chars)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Annahhthhhoohhthhioitoohoehhhohieihoheithhohoehhhhehhtothhthhhhoeieiihhehtohihhehiioehthehtethhhhehhhhhoioetoihhiehohitthhehheeeihheiteiehoiothiehhhhhehiioiheheohoieehhhiiehhtohehhhoiteihhohtehohhhhetehhotheeihhothoohothhhhoohteiihethohhhhhthhhooehhhhohhhioohehehthihhhhhttthihthhthihhioeeheheihhoihohhhhheohhthhitohhhehhihoehiihthohhhhhihhihhihohhihheithehohhohoothhhehehtehihhthhethihheetheohhhheehthhheehotethhotohhthiiihihoohhohoeeiiohththhthiehhhhhhtheotthitihteoeteiehhhhehhehoihttheoitehhotheehhohheethhhihhihhoeeohthhehtihoththhetthiihhthehhhhhthhioehhtiohttohhoehihetthietthhihhhtohohetihhhhoihhthttihoehehhtohoootothhhiihehhtehheohhhoiiehhhhhihthothhhtehhhhiooethththhhihhtiehhehhtohhhiiohohhihhhotiohehhehhhhheeetehehehtiiehhhhhteehtitthhehhihethheehhehhteeoohhhteoethhehhettehhetetoohihheihthhihthohhhieihhihehetohthehihhhoohhotthhoihheoehoihhhhhhohohhhthhhiihhtieehteethioihhthiihtoohhietohhhohetihhehhtheotteithhhhehhiiooieihehhohhihhiheehthithhhehehhohhhtheihhihiiiethehieohttohhhtoiohhtiieeih'"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 1000, prime ='Anna', top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "\n",
    "loaded = CharRNN(checkpoint['tokens'], checkpoint['n_hidden'], checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin 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\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
